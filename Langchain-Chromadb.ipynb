{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Langchain and ChromaDB\n",
    "\n",
    "This notebook uses Langchain and ChromaDB for a simple RAG example\n",
    "\n",
    "References:\n",
    "- [What is RAG?](https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag)\n",
    "- [What is a Vector Database?](https://learn.microsoft.com/en-us/semantic-kernel/memories/vector-db)\n",
    "- [Langchain and RAG](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [Langchain Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the current data/vectordb folder\n",
    "if os.path.exists(\"data/vectordb\"):\n",
    "    os.system(\"rm -rf data/vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "# We only care about the post content, title and header.\n",
    "bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into chunks of 1000 characters with 200 characters overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Model\n",
    "\n",
    "We need a different model to take care of embeddings. The following cell uses different embeddings. You need to uncomment the one you wish to use.\n",
    "The available embeddings are:\n",
    "- HuggingFace Sentence Transformer: [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2). You can pull any of the available Sentence Transformer Embeddings from https://huggingface.co/sentence-transformers\n",
    "- Ollama embedding: [nomic-embed-text](https://ollama.com/library/nomic-embed-text), [llama3](https://ollama.com/library/llama3). Ollama also provide embedding models. More information can be found at https://ollama.com/blog/embedding-models\n",
    "- GPT4All. This is a free embedding that will be downloaded when use the first time. https://python.langchain.com/docs/integrations/text_embedding/gpt4all/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "#from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "\n",
    "# Index all documents in a single vector store\n",
    "# This one is using the all-MiniLM-L6-v2 model for embedding\n",
    "embedding = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# This one is using the Ollama model for embedding, pick one of them\n",
    "# you will need to pull the model from the ollama server first\n",
    "#embedding = OllamaEmbeddings(model=\"nomic-embed-text\")  # special embedding from Ollama\n",
    "#embedding = OllamaEmbeddings(model=\"llama3:8b-instruct-q8_0\")\n",
    "\n",
    "# another open-source embedding function\n",
    "# embedding = GPT4AllEmbeddings()\n",
    "\n",
    "# Using the embeddings to index the documents in Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding, persist_directory=\"data/vectordb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "You can use any LLM for your RAG. Below are three different providers. Each provider has more than one model so you can select the model to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OLLAMA #####\n",
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3:8b-instruct-q8_0\", temperature=0)\n",
    "\n",
    "###### OPENAI #####\n",
    "# from langchain_openai.chat_models import ChatOpenAI\n",
    "# openai_models = [\"gpt-3.5-turbo-0125\", \"gpt-4-turbo\", \"gpt-4-turbo-preview\"]\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name=openai_models[0],\n",
    "#     temperature=0,\n",
    "#     api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "###### GROQ #####\n",
    "# from langchain_groq.chat_models import ChatGroq\n",
    "# groq_model = [\"mixtral-8x7b-32768\", \"gemma-7b-it\", \"llama2-70b-4096\", \"llama3-70b-8192\", \"llama3-8b-8192\"]\n",
    "# llm = ChatGroq(\n",
    "#     temperature=0,\n",
    "#     max_tokens=4096,\n",
    "#     model_name=groq_model[3], \n",
    "#     api_key = os.environ[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the chain to generate a response to a question. The answer will be generated and outputted.\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Documents\n",
    "The code below shows the documents retrieved to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Retriever and Ollama Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is Task Decomposition?\"\n",
    "docs_result = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "pprint(docs_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = format_docs(docs_result)\n",
    "pprint(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the prompt template to provide the context and question to the model.\n",
    "print(prompt)\n",
    "prompt_value = prompt.invoke(\n",
    "    {\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(prompt_value))\n",
    "print(prompt_value.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Generate a response using the Ollama model.\n",
    "model_name = 'llama3:8b-instruct-q8_0'\n",
    "ollama.generate(model=model_name, prompt=prompt_value.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
