{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain and Wikipedia Retriever\n",
    "\n",
    "This notebook uses Langchain and Wikipedia Retriever to answer questions\n",
    "\n",
    "Reference:\n",
    "- [Wikipedia Retriever](https://python.langchain.com/docs/integrations/retrievers/wikipedia/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### OLLAMA #####\n",
    "#from langchain_community.llms import Ollama\n",
    "#llm = Ollama(model=\"llama3:8b-instruct-q8_0\", temperature=0)\n",
    "\n",
    "###### OPENAI #####\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "openai_models = [\"gpt-3.5-turbo-0125\", \"gpt-4-turbo\", \"gpt-4-turbo-preview\"]\n",
    "llm = ChatOpenAI(\n",
    "    model_name=openai_models[0],\n",
    "    temperature=0,\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "###### GROQ #####\n",
    "# from langchain_groq.chat_models import ChatGroq\n",
    "# groq_model = [\"mixtral-8x7b-32768\", \"gemma-7b-it\", \"llama2-70b-4096\", \"llama3-70b-8192\", \"llama3-8b-8192\"]\n",
    "# llm = ChatGroq(\n",
    "#     temperature=0,\n",
    "#     max_tokens=4096,\n",
    "#     model_name=groq_model[3], \n",
    "#     api_key = os.environ[\"GROQ_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "wiki = WikipediaRetriever(top_k_results=6, doc_content_chars_max=2000)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You're a helpful AI assistant. Given a user question and some Wikipedia article snippets, answer the user question. If none of the articles answer the question, just say you don't know.\\n\\nHere are the Wikipedia articles:{context}\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import (\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Convert Documents to a single string.:\"\"\"\n",
    "    formatted = [\n",
    "        f\"Article Title: {doc.metadata['title']}\\nArticle Snippet: {doc.page_content}\"\n",
    "        for doc in docs\n",
    "    ]\n",
    "    return \"\\n\\n\" + \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "format = itemgetter(\"docs\") | RunnableLambda(format_docs)\n",
    "# subchain for generating an answer once we've done retrieval\n",
    "answer = prompt | llm | StrOutputParser()\n",
    "# complete chain that calls wiki -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.\n",
    "chain = (\n",
    "    RunnableParallel(question=RunnablePassthrough(), docs=wiki)\n",
    "    .assign(context=format)\n",
    "    .assign(answer=answer)\n",
    "    .pick([\"answer\", \"docs\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"How fast are cheetahs?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
